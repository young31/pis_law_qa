import streamlit as st
import os
import re
from pathlib import Path
import logging

# Import from our new modules
from config import *
from doc_processing import load_text_documents_from_directory, load_pdf_documents_from_directory, load_qa_from_excel
from llm_services import get_ollama_llm, get_gemini_llm, get_ollama_embeddings, get_google_embeddings, create_or_load_vectorstore
from rag_pipelines import generate_answer_with_rag, generate_blended_answer, generate_direct_llm_answer
from ui_components import display_results
# --- Main Streamlit Application ---
def main():
    st.set_page_config(page_title="AI ë²•ë¥  ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ", layout="wide", initial_sidebar_state="expanded")
    st.title("âš–ï¸ AI ë²•ë¥  ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

    # --- Session State Initialization ---
    session_defaults = {
        "llm": None,
        "llm_provider_name_loaded": "Ollama",
        "llm_model_name": DEFAULT_OLLAMA_LLM_MODEL,
        "embeddings": None,
        "embedding_provider_name_loaded": "Ollama",
        "embedding_model_name_loaded": DEFAULT_OLLAMA_EMBEDDING_MODEL,
        "vectorstore": None,
        "vectorstore_config_key_loaded": None,
        "db_status_message": "DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ê³  'DB ì¤€ë¹„' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "db_ready": False,
        "texts_with_sources_cache": None,
        "full_texts_map_cache": {},
        "last_loaded_doc_dir": DEFAULT_DOC_DIRECTORY,
        "last_loaded_file_type": "txt",
        "uploaded_qa_file_name": None,
        "qa_documents_cache": None,
        "sidebar_doc_dir": DEFAULT_DOC_DIRECTORY,
        "sidebar_file_type": "txt",
        "sidebar_emb_provider": "Ollama",
        "sidebar_ollama_emb_model": DEFAULT_OLLAMA_EMBEDDING_MODEL,
        "sidebar_google_emb_model": DEFAULT_GOOGLE_EMBEDDING_MODEL,
        "sidebar_llm_provider": "Ollama",
        "sidebar_ollama_llm_model": DEFAULT_OLLAMA_LLM_MODEL,
        "sidebar_gemini_llm_model": DEFAULT_GEMINI_LLM_MODEL,
        "sidebar_chunk_size": CHUNK_SIZE_DEFAULT,
        "sidebar_chunk_overlap": CHUNK_OVERLAP_DEFAULT,
        "sidebar_force_recreate_db": False,
    }
    for key, value in session_defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

    st.caption(f"LLM ({st.session_state.llm_provider_name_loaded}: {st.session_state.llm_model_name if st.session_state.llm else 'ë¡œë“œ ì•ˆë¨'}), "
               f"ì„ë² ë”© ({st.session_state.embedding_provider_name_loaded}: "
               f"{st.session_state.embedding_model_name_loaded if st.session_state.embeddings else 'ë¡œë“œ ì•ˆë¨'})")

    # --- Sidebar UI ---
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")

        st.subheader("1. ë²•ë¥  ë¬¸ì„œ ë¡œë”©")
        st.session_state.sidebar_doc_dir = st.text_input("ë¬¸ì„œ ë””ë ‰í† ë¦¬", value=st.session_state.sidebar_doc_dir, key="doc_dir_sb")
        st.session_state.sidebar_file_type = st.selectbox("ë¬¸ì„œ íƒ€ì…", ["txt", "pdf"], index=["txt", "pdf"].index(st.session_state.sidebar_file_type), key="file_type_sb")

        st.subheader("2. ì„ë² ë”© ëª¨ë¸")
        st.session_state.sidebar_emb_provider = st.selectbox("ì œê³µì", ["Ollama", "Google"], index=["Ollama", "Google"].index(st.session_state.sidebar_emb_provider), key="emb_prov_sb")
        current_embedding_model_to_use = ""
        if st.session_state.sidebar_emb_provider == "Ollama":
            st.session_state.sidebar_ollama_emb_model = st.text_input("Ollama ëª¨ë¸ëª…", value=st.session_state.sidebar_ollama_emb_model, key="ollama_emb_sb", help=f"ì˜ˆ: {DEFAULT_OLLAMA_EMBEDDING_MODEL}")
            current_embedding_model_to_use = st.session_state.sidebar_ollama_emb_model
        elif st.session_state.sidebar_emb_provider == "Google":
            st.session_state.sidebar_google_emb_model = st.text_input("Google ëª¨ë¸ëª…", value=st.session_state.sidebar_google_emb_model, key="google_emb_sb", disabled=not GOOGLE_API_KEY, help=f"ì˜ˆ: {DEFAULT_GOOGLE_EMBEDDING_MODEL}")
            if not GOOGLE_API_KEY: st.warning("âš ï¸ GOOGLE_API_KEY ì—†ìŒ.")
            current_embedding_model_to_use = st.session_state.sidebar_google_emb_model

        st.subheader("3. LLM (ë‹µë³€ ìƒì„±)")
        st.session_state.sidebar_llm_provider = st.selectbox(
            "LLM ì œê³µì", ["Ollama", "Gemini"],
            index=["Ollama", "Gemini"].index(st.session_state.sidebar_llm_provider),
            key="llm_provider_sb", help="ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  LLMì˜ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ì„¸ìš”."
        )
        if st.session_state.sidebar_llm_provider == "Ollama":
            st.session_state.sidebar_ollama_llm_model = st.text_input("Ollama LLM ëª¨ë¸ëª…", value=st.session_state.sidebar_ollama_llm_model, key="ollama_llm_sb", help=f"ì˜ˆ: {DEFAULT_OLLAMA_LLM_MODEL}")
        elif st.session_state.sidebar_llm_provider == "Gemini":
            st.session_state.sidebar_gemini_llm_model = st.text_input("Gemini LLM ëª¨ë¸ëª…", value=st.session_state.sidebar_gemini_llm_model, key="gemini_llm_sb", disabled=not GOOGLE_API_KEY, help="ì˜ˆ: gemini-2.0-flash, gemini-2.0-flash-lite-001")
            if not GOOGLE_API_KEY: st.warning("âš ï¸ GOOGLE_API_KEY ì—†ìŒ.")

        st.subheader("4. ë²¡í„° DB ì„¤ì •")
        st.session_state.sidebar_chunk_size = st.slider("Chunk Size", 200, 4000, st.session_state.sidebar_chunk_size, 50, key="chunk_size_sb")
        st.session_state.sidebar_chunk_overlap = st.slider("Chunk Overlap", 0, 1000, st.session_state.sidebar_chunk_overlap, 50, key="chunk_overlap_sb")
        
        st.subheader("5. ì´ì „ Q&A ë°ì´í„° (ì„ íƒ)")
        uploaded_qa_file_ui = st.file_uploader("Q&A ì°¸ê³ ìë£Œ Excel (.xlsx, .xls)", type=["xlsx", "xls"], key="qa_file_uploader_sb")
        
        st.session_state.sidebar_force_recreate_db = st.checkbox("DB ê°•ì œ ì¬ìƒì„±", value=st.session_state.sidebar_force_recreate_db, key="force_recreate_sb")

    # --- LLM Loading/Re-loading Logic ---
    desired_llm_provider = st.session_state.sidebar_llm_provider
    if desired_llm_provider == "Ollama":
        desired_llm_model = st.session_state.sidebar_ollama_llm_model
    else:  # Gemini
        desired_llm_model = st.session_state.sidebar_gemini_llm_model

    llm_needs_update = (
        st.session_state.llm is None or
        st.session_state.llm_provider_name_loaded != desired_llm_provider or
        st.session_state.llm_model_name != desired_llm_model
    )
    if llm_needs_update and desired_llm_model:
        with st.spinner(f"{desired_llm_provider} LLM ({desired_llm_model}) ë¡œë“œ ì¤‘..."):
            llm_instance = None
            if desired_llm_provider == "Ollama":
                llm_instance = get_ollama_llm(desired_llm_model)
            elif desired_llm_provider == "Gemini":
                llm_instance = get_gemini_llm(desired_llm_model)

            if llm_instance:
                st.session_state.llm = llm_instance
                st.session_state.llm_provider_name_loaded = desired_llm_provider
                st.session_state.llm_model_name = desired_llm_model
                logging.info(f"LLM switched/loaded: {desired_llm_provider} - {desired_llm_model}")
            else:
                if st.session_state.llm:
                    st.sidebar.error(f"ìƒˆ LLM({desired_llm_provider} - {desired_llm_model}) ë¡œë“œ ì‹¤íŒ¨. ì´ì „ LLMì„ ê³„ì† ì‚¬ìš©í•©ë‹ˆë‹¤.")
                else:
                    st.session_state.llm = None
                    st.session_state.llm_provider_name_loaded = desired_llm_provider
                    st.session_state.llm_model_name = desired_llm_model
                    st.sidebar.error(f"LLM({desired_llm_provider} - {desired_llm_model}) ë¡œë“œ ì‹¤íŒ¨. ë‹µë³€ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")

    if st.session_state.llm:
        st.sidebar.success(f"LLM ({st.session_state.llm_provider_name_loaded}: {st.session_state.llm_model_name}) ì¤€ë¹„ ì™„ë£Œ")
    else:
        failed_provider = st.session_state.sidebar_llm_provider
        failed_model = st.session_state.sidebar_ollama_llm_model if failed_provider == "Ollama" else st.session_state.sidebar_gemini_llm_model
        st.sidebar.error(f"LLM ({failed_provider}: {failed_model}) ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ì•ˆë¨.")

    # --- Main Area ---
    col1, col2 = st.columns([1.2, 2])
    with col1:
        st.subheader("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬")
        st.info(st.session_state.db_status_message)

        safe_sidebar_emb_model_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', current_embedding_model_to_use).replace('/', '_').replace(':','_')
        current_sidebar_db_config_key = (f"chroma_db_{st.session_state.sidebar_emb_provider.lower()}_{safe_sidebar_emb_model_name}_cz{st.session_state.sidebar_chunk_size}_co{st.session_state.sidebar_chunk_overlap}")
        
        if st.session_state.db_ready and st.session_state.vectorstore_config_key_loaded != current_sidebar_db_config_key:
            st.warning("âš ï¸ DB ì„¤ì • ë³€ê²½ë¨. DB ì¬ë¡œë“œ í•„ìš”.")

        button_text = f"'{st.session_state.sidebar_doc_dir}'({st.session_state.sidebar_file_type.upper()}) ë¡œë“œ ë° DB ì¤€ë¹„"
        if st.session_state.sidebar_force_recreate_db: button_text += " (ê°•ì œ ì¬ìƒì„±)"
        
        if st.button(button_text, key="prepare_db_main", use_container_width=True):
            st.session_state.db_ready = False
            st.session_state.db_status_message = "DB ì²˜ë¦¬ ì‹œì‘..."
            
            # 1. Initialize Embeddings
            embeddings_instance, emb_init_success = None, False
            with st.spinner(f"{st.session_state.sidebar_emb_provider} ì„ë² ë”© ({current_embedding_model_to_use}) ì´ˆê¸°í™”..."):
                if st.session_state.sidebar_emb_provider == "Ollama":
                    if current_embedding_model_to_use: embeddings_instance = get_ollama_embeddings(current_embedding_model_to_use)
                    else: st.error("Ollama ì„ë² ë”© ëª¨ë¸ëª… ë¯¸ì§€ì •.")
                elif st.session_state.sidebar_emb_provider == "Google":
                    if not GOOGLE_API_KEY: st.error("Google API í‚¤ ì—†ìŒ.")
                    elif current_embedding_model_to_use: embeddings_instance = get_google_embeddings(current_embedding_model_to_use)
                    else: st.error("Google ì„ë² ë”© ëª¨ë¸ëª… ë¯¸ì§€ì •.")
                if embeddings_instance: emb_init_success = True
            
            if not emb_init_success:
                st.session_state.db_status_message = "âŒ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨."
                st.session_state.embeddings = None
                st.rerun()

            st.session_state.embeddings = embeddings_instance
            st.session_state.embedding_provider_name_loaded = st.session_state.sidebar_emb_provider
            st.session_state.embedding_model_name_loaded = current_embedding_model_to_use
            logging.info(f"Embeddings set: {st.session_state.embedding_provider_name_loaded} - {st.session_state.embedding_model_name_loaded}")
            
            # 2. Load Q&A documents
            current_qa_docs = []
            if uploaded_qa_file_ui is not None:
                if (st.session_state.uploaded_qa_file_name != uploaded_qa_file_ui.name) or \
                   st.session_state.qa_documents_cache is None or st.session_state.sidebar_force_recreate_db:
                    with st.spinner(f"'{uploaded_qa_file_ui.name}' Q&A ë°ì´í„° ë¡œë“œ..."):
                        loaded_qa_docs = load_qa_from_excel(uploaded_qa_file_ui)
                    st.session_state.qa_documents_cache = loaded_qa_docs if loaded_qa_docs else []
                    st.session_state.uploaded_qa_file_name = uploaded_qa_file_ui.name
                    if loaded_qa_docs: st.success(f"'{uploaded_qa_file_ui.name}'ì—ì„œ {len(loaded_qa_docs)}ê°œ Q&A ë¡œë“œ ì™„ë£Œ.")
                current_qa_docs = st.session_state.qa_documents_cache
            else:
                if st.session_state.uploaded_qa_file_name is not None: st.info("Q&A Excel íŒŒì¼ ì œê±°ë¨.")
                st.session_state.qa_documents_cache, st.session_state.uploaded_qa_file_name = [], None

            current_qa_docs = st.session_state.qa_documents_cache if st.session_state.qa_documents_cache else []

            # 3. Load primary documents
            texts_sources_for_db = None
            should_load_fresh_docs = (st.session_state.last_loaded_doc_dir != st.session_state.sidebar_doc_dir or
                                      st.session_state.last_loaded_file_type != st.session_state.sidebar_file_type or
                                      st.session_state.texts_with_sources_cache is None or st.session_state.sidebar_force_recreate_db)
            if should_load_fresh_docs:
                if not os.path.isdir(st.session_state.sidebar_doc_dir):
                    st.error(f"ë¬¸ì„œ ë””ë ‰í† ë¦¬ '{st.session_state.sidebar_doc_dir}' ì°¾ì„ ìˆ˜ ì—†ìŒ."); st.session_state.texts_with_sources_cache = None; st.rerun()
                load_func = load_text_documents_from_directory if st.session_state.sidebar_file_type == "txt" else load_pdf_documents_from_directory
                with st.spinner(f"'{st.session_state.sidebar_doc_dir}' ({st.session_state.sidebar_file_type.upper()}) ë¡œë“œ..."):
                    texts_sources_for_db = load_func(st.session_state.sidebar_doc_dir)
                if texts_sources_for_db:
                    st.session_state.texts_with_sources_cache = texts_sources_for_db
                    st.session_state.full_texts_map_cache = dict(texts_sources_for_db)
                    st.session_state.last_loaded_doc_dir, st.session_state.last_loaded_file_type = st.session_state.sidebar_doc_dir, st.session_state.sidebar_file_type
                else: st.warning(f"'{st.session_state.sidebar_doc_dir}' ({st.session_state.sidebar_file_type})ì—ì„œ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨."); st.session_state.texts_with_sources_cache = None; st.session_state.full_texts_map_cache = {}; st.rerun()
            else:
                texts_sources_for_db = st.session_state.texts_with_sources_cache
                if texts_sources_for_db: st.info(f"ìºì‹œëœ ë²•ë¥  ë¬¸ì„œ {len(texts_sources_for_db)}ê°œ ì‚¬ìš©.")

            # 4. Create/Load Vectorstore
            if (texts_sources_for_db or current_qa_docs) or not st.session_state.sidebar_force_recreate_db:
                with st.spinner("ë²¡í„° DB êµ¬ì„± ì¤‘... (ì‹œê°„ ì†Œìš”ë  ìˆ˜ ìˆìŒ)"):
                    vector_db_instance = create_or_load_vectorstore(
                        texts_sources_for_db, current_qa_docs, st.session_state.embeddings,
                        st.session_state.embedding_provider_name_loaded, st.session_state.embedding_model_name_loaded,
                        st.session_state.sidebar_chunk_size, st.session_state.sidebar_chunk_overlap,
                        st.session_state.sidebar_force_recreate_db
                    )
                if vector_db_instance:
                    st.session_state.vectorstore, st.session_state.vectorstore_config_key_loaded, st.session_state.db_ready = vector_db_instance, current_sidebar_db_config_key, True
                    st.session_state.db_status_message = f"âœ… DB ì¤€ë¹„ ì™„ë£Œ: {st.session_state.embedding_model_name_loaded} (Chunk: {st.session_state.sidebar_chunk_size}/{st.session_state.sidebar_chunk_overlap})"
                    if st.session_state.sidebar_force_recreate_db: st.session_state.sidebar_force_recreate_db = False
                else:
                    st.session_state.vectorstore, st.session_state.db_ready = None, False
                    st.session_state.db_status_message = "âŒ ë²¡í„° DB ì¤€ë¹„ ì‹¤íŒ¨. ë¡œê·¸ í™•ì¸."
            else:
                st.session_state.db_status_message = "âŒ DB ê°•ì œ ì¬ìƒì„±: ì¶”ê°€í•  ë²•ë¥ /Q&A ë°ì´í„° ì—†ìŒ."
            st.rerun()

    with col2:
        st.subheader("ğŸ’¬ ì§ˆì˜ì‘ë‹µ")
        answer_mode = st.radio("ë‹µë³€ ë°©ì‹:", ["RAG (ë¬¸ì„œ ê¸°ë°˜)", "Blended (RAG + LLM ê°œì„ )", "LLM ì§ì ‘ ë‹µë³€"], index=1, key="answer_mode_main", horizontal=True,
                               help=("**RAG**: ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€.\n**Blended**: RAG ë‹µë³€ì„ ê°œì„ í•˜ê³ , ì‹¤íŒ¨ ì‹œ ì „ì²´ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì¬ì‹œë„.\n**LLM ì§ì ‘ ë‹µë³€**: LLM ìì²´ ì§€ì‹ìœ¼ë¡œ ë‹µë³€."))
        
        llm_ready = bool(st.session_state.llm)
        db_compatible = st.session_state.db_ready and st.session_state.vectorstore is not None and (st.session_state.vectorstore_config_key_loaded == current_sidebar_db_config_key)
        can_ask = False

        if not llm_ready:
            st.warning("LLMì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°” ì„¤ì • í™•ì¸ í›„ ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        
        if answer_mode == "LLM ì§ì ‘ ë‹µë³€":
            if llm_ready: can_ask = True
        else:  # RAG or Blended
            if not llm_ready: pass
            elif not st.session_state.db_ready: st.warning("DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì™¼ìª½ 'DB ì¤€ë¹„' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
            elif not db_compatible: st.warning("ë¡œë“œëœ DBê°€ í˜„ì¬ ì‚¬ì´ë“œë°” ì„¤ì •ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. DBë¥¼ í˜„ì¬ ì„¤ì •ìœ¼ë¡œ ë‹¤ì‹œ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
            else: can_ask = True

        question = st.text_area("ì§ˆë¬¸ ì…ë ¥:", height=100, key="user_question_main", placeholder="ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ ì…ë ¥...", disabled=not can_ask)
        
        if st.button("ë‹µë³€ ìƒì„± ğŸš€", key="ask_btn_main", disabled=not can_ask or not question.strip(), use_container_width=True):
            with st.spinner("AI ë‹µë³€ ìƒì„± ì¤‘... ğŸ¤”"):
                final_result_data = None
                if answer_mode != "LLM ì§ì ‘ ë‹µë³€" and not st.session_state.vectorstore: st.error("ë²¡í„°ìŠ¤í† ì–´ ë¯¸ì¤€ë¹„.")
                elif not st.session_state.llm: st.error("LLM ë¯¸ì¤€ë¹„.")
                else:
                    if answer_mode == "RAG (ë¬¸ì„œ ê¸°ë°˜)":
                        final_result_data = generate_answer_with_rag(question, st.session_state.vectorstore, st.session_state.llm)
                    elif answer_mode == "Blended (RAG + LLM ê°œì„ )":
                        full_texts_map = st.session_state.get("full_texts_map_cache", {})
                        final_result_data = generate_blended_answer(question, st.session_state.vectorstore, st.session_state.llm, full_texts_map)
                    elif answer_mode == "LLM ì§ì ‘ ë‹µë³€":
                        final_result_data = generate_direct_llm_answer(question, st.session_state.llm)
                
                if final_result_data:
                    display_results(final_result_data, answer_mode)
        elif not question.strip() and can_ask:
            st.info("ë‹µë³€ì„ ìƒì„±í•˜ë ¤ë©´ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    Path(DEFAULT_DOC_DIRECTORY).mkdir(parents=True, exist_ok=True)
    logging.info(f"Default document directory ensured: '{DEFAULT_DOC_DIRECTORY}'")
    main()