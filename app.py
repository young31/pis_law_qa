# -*- coding: utf-8 -*-
import streamlit as st
import os
import re
import logging
from pathlib import Path
import dotenv
from typing import List, Tuple, Dict, Optional, Any
import time
import pandas as pd

# LangChain components
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader
from langchain_core.embeddings import Embeddings
from langchain_core.messages import BaseMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
)
dotenv.load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
else:
    logging.warning("GOOGLE_API_KEY environment variable not found.")

# --- Constants ---
DEFAULT_DOC_DIRECTORY = "./laws_data_streamlit_app"
DEFAULT_OLLAMA_EMBEDDING_MODEL = "mxbai-embed-large" # or "nomic-embed-text"
DEFAULT_GOOGLE_EMBEDDING_MODEL = "models/embedding-001"
DEFAULT_OLLAMA_LLM_MODEL = "phi4-mini" #  Adjust if needed, e.g., "llama3", "qwen2"
CHUNK_SIZE_DEFAULT = 800
CHUNK_OVERLAP_DEFAULT = 150

DEFAULT_RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ë²•ë¥  ì¡°í•­ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì˜¤ì§ ì£¼ì–´ì§„ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì£¼ì–´ì§„ ë¬¸ë§¥ì—ëŠ” ê´€ë ¨ ë²•ë¥  ë¬¸ì„œì˜ ì¼ë¶€ì™€ í•¨ê»˜, ì°¸ê³ í•  ë§Œí•œ ì´ì „ ì§ˆì˜ì‘ë‹µ(Q&A) ê¸°ë¡ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ Q&A ê¸°ë¡ì€ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê±°ë‚˜ ë‹µë³€ì˜ ë°©í–¥ì„ ì¡ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì ê·¹ì ìœ¼ë¡œ ì°¸ê³ í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ ë²•ë¥  ë¬¸ì„œ ë° Q&A ë¬¸ë§¥ì— ê·¼ê±°í•´ì•¼ í•˜ë©°, ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ë¡œ ì–¸ê¸‰í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
ë§Œì•½ ë¬¸ë§¥ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ë§¥ ì •ë³´ë§Œìœ¼ë¡œëŠ” ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
ê°€ëŠ¥í•˜ë‹¤ë©´ ê´€ë ¨ ë²• ì¡°í•­ì˜ ì œëª©ì´ë‚˜ ë²ˆí˜¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”. Q&A ë‚´ìš©ì„ ì°¸ê³ í–ˆë‹¤ë©´, ê·¸ ì‚¬ì‹¤ì„ ì§ì ‘ì ìœ¼ë¡œ ì–¸ê¸‰í•  í•„ìš”ëŠ” ì—†ìœ¼ë‚˜ ë‹µë³€ ë‚´ìš©ì— ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜í•´ì£¼ì„¸ìš”.

ë¬¸ë§¥:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ (í•œêµ­ì–´, ë¬¸ë§¥ ê·¼ê±°):"""

REFINEMENT_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ AI ë²•ë¥  ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ(ë¬¸ë§¥)ì™€ Q&A ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ ì´ˆê¸° ë‹µë³€ì´ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤. ì´ ì´ˆê¸° ë‹µë³€ê³¼ ë¬¸ë§¥ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬, ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì— ëŒ€í•´ ë” ì •í™•í•˜ê³ , í¬ê´„ì ì´ë©°, ìì—°ìŠ¤ëŸ¬ìš´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.
ìµœì¢… ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ **ë¬¸ë§¥ ì •ë³´**(ë²•ë¥  ë¬¸ì„œ, Q&A ê¸°ë¡ í¬í•¨)ì™€ **ì´ˆê¸° ë‹µë³€**ì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
ë‹µë³€ì˜ íë¦„ì„ ê°œì„ í•˜ê³ , ëª…í™•ì„±ì„ ë†’ì´ì„¸ìš”. ê´€ë ¨ëœ ë²• ì¡°í•­ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤. ì´ˆê¸° ë‹µë³€ì´ "ì œê³µëœ ë¬¸ë§¥ ì •ë³´ë§Œìœ¼ë¡œëŠ” ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ì™€ ê°™ì´ ë‹µë³€ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ìµœì¢… ë‹µë³€ë„ ë™ì¼í•˜ê²Œ ë˜ëŠ” ìœ ì‚¬í•œ ì˜ë¯¸ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ìµœì¢… ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ì›ë³¸ ì§ˆë¬¸:** {original_question}

**ê²€ìƒ‰ëœ ë¬¸ë§¥ ì •ë³´:**
{retrieved_context}

**ì´ˆê¸° RAG ë‹µë³€:**
{rag_answer}

**ê°œì„ ëœ ìµœì¢… ë‹µë³€ (í•œêµ­ì–´, ë¬¸ë§¥ ë° ì´ˆê¸° ë‹µë³€ ê·¼ê±°):**"""

# --- Helper Functions ---
@st.cache_data(show_spinner="í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì¤‘...")
def load_text_documents_from_directory(directory_path: str) -> List[Tuple[str, str]]:
    return _load_documents(directory_path, "txt")

@st.cache_data(show_spinner="PDF íŒŒì¼ ë¡œë“œ ì¤‘...")
def load_pdf_documents_from_directory(directory_path: str) -> List[Tuple[str, str]]:
    return _load_documents(directory_path, "pdf")

def _load_documents(directory_path: str, file_type: str) -> List[Tuple[str, str]]:
    if not os.path.isdir(directory_path):
        logging.error(f"Directory not found: {directory_path}")
        st.error(f"ë””ë ‰í† ë¦¬ '{directory_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ê²½ë¡œì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return []
    glob_pattern = f"**/*.{file_type.lower()}"
    try:
        common_loader_params = {"recursive": True, "show_progress": False, "use_multithreading": True, "silent_errors": True}
        if file_type == "pdf":
            loader = DirectoryLoader(directory_path, glob=glob_pattern, loader_cls=PyPDFLoader, **common_loader_params)
        elif file_type == "txt":
            loader = DirectoryLoader(directory_path, glob=glob_pattern, loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8', 'autodetect_encoding': True}, **common_loader_params)
        else:
            logging.error(f"Unsupported file type: {file_type}"); st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ íƒ€ì…ì…ë‹ˆë‹¤: {file_type}"); return []
        logging.info(f"Loading {file_type} documents from '{directory_path}' using glob: '{glob_pattern}'")
        docs = loader.load()
        logging.info(f"Loaded {len(docs)} document sections initially from DirectoryLoader.")
        if not docs:
            logging.warning(f"No documents loaded from '{directory_path}' with pattern '{glob_pattern}'."); st.warning(f"'{directory_path}'ì—ì„œ '{glob_pattern}' íŒ¨í„´ìœ¼ë¡œ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."); return []
    except Exception as e:
        logging.error(f"Error during document loading from {directory_path} (file_type: {file_type}): {e}", exc_info=True); st.error(f"ë””ë ‰í† ë¦¬ '{directory_path}'ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}"); return []

    docs_by_source: Dict[str, List[str]] = {}
    for doc in docs:
        source = doc.metadata.get('source', 'Unknown Source'); page_content = doc.page_content if doc.page_content else ""
        if source not in docs_by_source: docs_by_source[source] = []
        docs_by_source[source].append(page_content)

    texts_with_sources: List[Tuple[str, str]] = []
    for source, content_list in docs_by_source.items():
        full_text = "\n".join(content_list).strip()
        if not full_text:
            logging.warning(f"File '{Path(source).name}' resulted in empty text. Skipping.")
            continue
        texts_with_sources.append((Path(source).name, full_text))

    logging.info(f"Extracted text from {len(texts_with_sources)} source files.")
    if not texts_with_sources: st.warning("ë¡œë“œëœ íŒŒì¼ì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return texts_with_sources

def preprocess_text(text: str) -> str:
    if not isinstance(text, str):
        logging.warning(f"preprocess_text received non-string: {type(text)}")
        return ""
    lines = text.strip().split("\n"); cleaned_lines = []
    for line in lines:
        line = line.strip()
        if not line: continue
        if "ë²•ì œì²˜ êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°" in line or "www.law.go.kr" in line or "National Law Information Center" in line: continue
        if re.match(r'^\s*page\s*\d+\s*/\s*\d+\s*$', line.lower()): continue
        line = re.sub(r'<[^>]+>', '', line); line = re.sub(r'\[.*?\]', '', line) # Remove HTML tags and bracketed content
        if re.match(r'^\s*ì œ\d+ì¡°(?:ì˜\d+)?\s+(?:<ì‚­ì œ>|ì‚­ì œ)\s*(\(\s*\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.\s*\))?$', line.strip()) or \
           re.match(r'^\s*\d+\.\s+(?:<ì‚­ì œ>|ì‚­ì œ)\s*$', line.strip()): continue
        line = re.sub(r'\b\d{2,4}-\d{3,4}-\d{4}\b', '', line) # Remove phone numbers
        line = re.sub(r'https?://\S+', '', line) # Remove URLs
        line = re.sub(r'\s+', ' ', line).strip() # Normalize whitespace
        if line: cleaned_lines.append(line)
    return "\n".join(cleaned_lines)

def split_text_by_article(processed_text: str) -> Dict[str, str]:
    articles: Dict[str, str] = {}
    current_article_title: Optional[str] = None
    current_article_content: List[str] = []
    article_start_pattern = re.compile(r'^(ì œ\s*(\d+(?:ì¡°)?(?:ì˜\s*\d+)?)(?:\s*ì¡°)?\s*\(([^)]+)\))\s*(.*)', re.IGNORECASE)
    chapter_pattern = re.compile(r'^ì œ\s*\d+\s*ì¥\s+.*', re.IGNORECASE)
    annex_pattern = re.compile(r'^(ë¶€\s*ì¹™|ë³„\s*í‘œ)', re.IGNORECASE)

    lines = processed_text.strip().split('\n')
    for line_num, line in enumerate(lines):
        line = line.strip()
        if not line: continue

        if annex_pattern.match(line):
            if current_article_title and current_article_content:
                article_text = "\n".join(current_article_content).strip()
                if article_text and article_text != current_article_title:
                    articles[current_article_title] = article_text
            current_article_title = None
            current_article_content = []
            logging.debug(f"Annex/Appendix found: '{line}'. Stopping article splitting for this section.")
            continue

        match = article_start_pattern.match(line)
        if match:
            if current_article_title and current_article_content:
                article_text = "\n".join(current_article_content).strip()
                if article_text and article_text != current_article_title:
                    articles[current_article_title] = article_text
            current_article_title = match.group(1).strip()
            initial_content_on_same_line = match.group(4).strip()
            current_article_content = [current_article_title]
            if initial_content_on_same_line:
                current_article_content.append(initial_content_on_same_line)
            logging.debug(f"Started new article: '{current_article_title}'")
        elif current_article_title:
            if chapter_pattern.match(line):
                logging.debug(f"Skipping chapter line '{line}' within article '{current_article_title}'")
                continue
            current_article_content.append(line)

    if current_article_title and current_article_content:
        article_text = "\n".join(current_article_content).strip()
        if article_text and article_text != current_article_title:
             articles[current_article_title] = article_text
    articles = {title: content for title, content in articles.items() if content.strip() and content != title}
    if not articles and processed_text.strip():
        logging.warning("Article splitting did not find specific articles. Using 'ì „ì²´ ë¬¸ì„œ' as a single article.")
        articles["ì „ì²´ ë¬¸ì„œ"] = processed_text
    elif not articles:
        logging.info("No articles found after splitting.")
    else:
        logging.info(f"Split text into {len(articles)} articles.")
    return articles

def extract_metadata_from_title(article_title: str) -> Dict[str, str]:
    metadata = {"ì¡°_ë²ˆí˜¸": "N/A", "ì¡°_ì œëª©": "ë‚´ìš© ì—†ìŒ"}
    if article_title == "ì „ì²´ ë¬¸ì„œ":
        return {"ì¡°_ë²ˆí˜¸": "N/A", "ì¡°_ì œëª©": "ì „ì²´ ë¬¸ì„œ"}
    match = re.match(r'ì œ\s*(\d+(?:ì¡°)?(?:ì˜\s*\d+)?)?(?:\s*ì¡°)?\s*\(([^)]+)\)', article_title, re.IGNORECASE)
    if match:
        raw_number_part = match.group(1)
        if raw_number_part: metadata["ì¡°_ë²ˆí˜¸"] = re.sub(r'[ì¡°\s]', '', raw_number_part).strip()
        else: metadata["ì¡°_ë²ˆí˜¸"] = "ë²ˆí˜¸ ë¶ˆëª…"
        title_candidate = match.group(2).strip().rstrip(')')
        metadata["ì¡°_ì œëª©"] = title_candidate if title_candidate else "ì œëª© ì—†ìŒ"
    else:
        logging.warning(f"Could not parse article number/title from: '{article_title}'."); metadata["ì¡°_ì œëª©"] = article_title
    return metadata

@st.cache_data(show_spinner="ì´ì „ Q&A ë°ì´í„° ë¡œë“œ ì¤‘...")
def load_qa_from_excel(uploaded_file) -> List[Document]:
    if uploaded_file is None:
        return []
    try:
        df = pd.read_excel(uploaded_file)
        qa_docs = []
        possible_q_cols = ["Question", "ì§ˆë¬¸", "ì§ˆì˜", "ì§ˆë¬¸ ë‚´ìš©"]
        possible_a_cols = ["Answer", "ë‹µë³€", "ì‘ë‹µ", "ë‹µë³€ ë‚´ìš©"]
        question_col, answer_col = None, None

        for col in df.columns:
            if col in possible_q_cols: question_col = col; break
        for col in df.columns:
            if col in possible_a_cols: answer_col = col; break

        if not question_col or not answer_col:
            st.error(f"Excel íŒŒì¼ì—ì„œ ì§ˆë¬¸ ì»¬ëŸ¼({', '.join(possible_q_cols)}) ë˜ëŠ” ë‹µë³€ ì»¬ëŸ¼({', '.join(possible_a_cols)})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        for index, row in df.iterrows():
            question = str(row[question_col]) if pd.notna(row[question_col]) else ""
            answer = str(row[answer_col]) if pd.notna(row[answer_col]) else ""
            if not question.strip() or not answer.strip():
                logging.warning(f"Skipping empty Q or A at Excel row {index+2} in '{uploaded_file.name}'")
                continue
            page_content = f"ì´ì „ ì§ˆë¬¸: {question}\nì´ì „ ë‹µë³€: {answer}"
            metadata = {
                "source_file": uploaded_file.name,
                "doc_type": "qa_pair",
                "original_question": question,
                "row_number": index + 2
            }
            for col_name in df.columns:
                if col_name not in [question_col, answer_col] and pd.notna(row[col_name]):
                    metadata[f"excel_{col_name.lower().replace(' ', '_')}"] = str(row[col_name])
            qa_docs.append(Document(page_content=page_content, metadata=metadata))

        logging.info(f"Loaded {len(qa_docs)} Q&A pairs from '{uploaded_file.name}'")
        if qa_docs: pass
        elif df.empty: st.warning(f"'{uploaded_file.name}' íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else: st.warning(f"'{uploaded_file.name}'ì—ì„œ ìœ íš¨í•œ Q&A ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸/ë‹µë³€ ë‚´ìš© ë˜ëŠ” ì»¬ëŸ¼ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return qa_docs
    except Exception as e:
        logging.error(f"Error loading Q&A from Excel ('{uploaded_file.name}'): {e}", exc_info=True)
        st.error(f"Excel íŒŒì¼ ('{uploaded_file.name}') ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

@st.cache_resource(show_spinner="Ollama ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
def get_ollama_embeddings(model_name: str) -> Optional[OllamaEmbeddings]:
    try:
        embeddings = OllamaEmbeddings(model=model_name)
        embeddings.embed_query("test")
        logging.info(f"Ollama Embeddings '{model_name}' initialized successfully.")
        return embeddings
    except Exception as e:
        logging.error(f"Ollama Embeddings '{model_name}' initialization FAIL: {e}", exc_info=True)
        st.error(f"Ollama ì„ë² ë”© ëª¨ë¸ ('{model_name}') ë¡œë“œ ì‹¤íŒ¨: {e}. Ollama ì„œë²„ ë° ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None

@st.cache_resource(show_spinner="Google AI ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
def get_google_embeddings(model_name: str) -> Optional[GoogleGenerativeAIEmbeddings]:
    if not GOOGLE_API_KEY:
        st.error("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        logging.error("Google API key not found when attempting to initialize Google Embeddings.")
        return None
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model=model_name)
        embeddings.embed_query("test")
        logging.info(f"Google AI Embeddings '{model_name}' initialized successfully.")
        return embeddings
    except Exception as e:
        logging.error(f"Google AI Embeddings '{model_name}' initialization FAIL: {e}", exc_info=True)
        st.error(f"Google AI ì„ë² ë”© ëª¨ë¸ ('{model_name}') ë¡œë“œ ì‹¤íŒ¨: {e}. API í‚¤ì™€ ëª¨ë¸ëª…ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

@st.cache_resource(show_spinner="Ollama LLM ë¡œë“œ ì¤‘...")
def get_ollama_llm(model_name: str, temperature: float = 0.1) -> Optional[ChatOllama]:
    try:
        llm = ChatOllama(model=model_name, temperature=temperature)
        llm.invoke("ì•ˆë…•")
        logging.info(f"Ollama LLM '{model_name}' initialized successfully.")
        return llm
    except Exception as e:
        logging.error(f"Ollama LLM '{model_name}' initialization FAIL: {e}", exc_info=True)
        st.error(f"Ollama LLM ('{model_name}') ë¡œë“œ ì‹¤íŒ¨: {e}. Ollama ì„œë²„ ë° ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None

# --- Vector Store Creation/Loading ---
def create_or_load_vectorstore(
    texts_with_sources: List[Tuple[str, str]],
    qa_documents: List[Document],
    embeddings: Embeddings,
    embedding_provider_name: str,
    embedding_model_name: str,
    chunk_size: int,
    chunk_overlap: int,
    force_recreate: bool = False
) -> Optional[Chroma]:
    overall_start_time = time.time()
    final_documents_for_db: List[Document] = []

    doc_processing_start_time = time.time()
    if texts_with_sources:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len, add_start_index=True,
            separators=["\n\n", "\n", ". ", " ", "", "ì œ\d+ì¡°", "ì œ\d+í•­"]
        )
        logging.info("Starting document processing for primary vector store sources...")
        primary_doc_chunks_count = 0
        for source_idx, (source_name, raw_text) in enumerate(texts_with_sources):
            if not raw_text.strip(): logging.warning(f"Source file '{source_name}' (idx {source_idx}) is empty. Skipping."); continue
            processed_text = preprocess_text(raw_text)
            if not processed_text.strip(): logging.warning(f"Preprocessing resulted in empty text for '{source_name}'. Skipping."); continue
            article_sections = split_text_by_article(processed_text)
            if not article_sections: logging.warning(f"No articles found in '{source_name}'.")
            for article_title, article_content in article_sections.items():
                if not article_content.strip(): logging.debug(f"Skipping empty article content for title '{article_title}' in '{source_name}'."); continue
                base_metadata = extract_metadata_from_title(article_title)
                base_metadata.update({"source_file": source_name, "article_title_full": article_title, "doc_type": "legal_document_chunk"})
                article_doc = Document(page_content=article_content, metadata=base_metadata)
                split_article_chunks = text_splitter.split_documents([article_doc])
                for i, chunk_doc in enumerate(split_article_chunks):
                    chunk_doc.metadata["chunk_index_in_article"] = i
                    final_documents_for_db.append(chunk_doc)
                    primary_doc_chunks_count += 1
        logging.info(f"Total legal document chunks created: {primary_doc_chunks_count}")
    doc_processing_end_time = time.time()
    if texts_with_sources:
        logging.info(f"Primary document processing took: {doc_processing_end_time - doc_processing_start_time:.2f}s")

    if qa_documents:
        final_documents_for_db.extend(qa_documents)
        logging.info(f"Added {len(qa_documents)} Q&A documents to the list for vector store.")
    logging.info(f"Total documents prepared for vector store: {len(final_documents_for_db)}")

    if not final_documents_for_db:
        st.error("ë¬¸ì„œ ë˜ëŠ” Q&A ë°ì´í„° ì²˜ë¦¬ í›„ ìƒì„±ëœ ë‚´ìš©ì´ ì—†ì–´ DBë¥¼ ë§Œë“¤ê±°ë‚˜ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logging.error("No documents (neither primary nor QA) to process for DB.")
        return None

    safe_model_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', embedding_model_name).replace('/', '_').replace(':', '_')
    persist_directory_name = f"chroma_db_{embedding_provider_name.lower()}_{safe_model_name}_cz{chunk_size}_co{chunk_overlap}"
    persist_directory = Path(persist_directory_name)
    logging.info(f"Target ChromaDB persistence directory: {persist_directory.resolve()}")
    vector_db: Optional[Chroma] = None

    if force_recreate and persist_directory.exists():
        logging.info(f"Force Recreate: Deleting existing ChromaDB at {persist_directory}")
        import shutil
        try: shutil.rmtree(persist_directory); logging.info(f"Deleted DB: {persist_directory}")
        except Exception as e: logging.error(f"Error deleting DB: {e}", exc_info=True); st.error(f"DB ì‚­ì œ ì˜¤ë¥˜: {e}"); return None

    if not force_recreate and persist_directory.exists() and any(persist_directory.iterdir()):
        load_db_start_time = time.time()
        try:
            logging.info(f"Attempting to load existing ChromaDB from {persist_directory}...")
            vector_db = Chroma(persist_directory=str(persist_directory), embedding_function=embeddings)
            if vector_db._collection.count() == 0: logging.warning(f"Existing DB at {persist_directory} is empty."); vector_db = None
            else: logging.info(f"Existing ChromaDB loaded ({vector_db._collection.count()} items). Took {time.time() - load_db_start_time:.2f}s.")
        except Exception as e:
            logging.error(f"Error loading ChromaDB: {e}", exc_info=True); st.warning(f"DB ë¡œë”© ì˜¤ë¥˜: {e}. ìƒˆ DB ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤."); vector_db = None

    if vector_db is None:
        if not final_documents_for_db: logging.warning("No documents for new DB."); return None
        create_db_start_time = time.time()
        try:
            logging.info(f"Creating new ChromaDB ({len(final_documents_for_db)} chunks)...")
            persist_directory.mkdir(parents=True, exist_ok=True)
            vector_db = Chroma.from_documents(documents=final_documents_for_db, embedding=embeddings, persist_directory=str(persist_directory))
            logging.info(f"New ChromaDB created ({vector_db._collection.count()} items). Took {time.time() - create_db_start_time:.2f}s.")
        except Exception as e:
            logging.error(f"Fatal error creating new ChromaDB: {e}", exc_info=True); st.error(f"ìƒˆ DB ìƒì„± ì‹¤íŒ¨: {e}")
            if persist_directory.exists(): import shutil; shutil.rmtree(persist_directory)
            return None
    logging.info(f"Total create_or_load_vectorstore took: {time.time() - overall_start_time:.2f}s.")
    return vector_db

# --- RAG & LLM Interaction Functions ---
def format_docs_for_context(docs: List[Document]) -> str:
    if not docs: return "ì œê³µëœ ë¬¸ë§¥ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    formatted_docs_list = []
    for i, doc in enumerate(docs):
        doc_type = doc.metadata.get('doc_type', 'unknown')
        header, content_preview = "", ""
        if doc_type == "qa_pair":
            original_q = doc.metadata.get('original_question', 'ì§ˆë¬¸ ì •ë³´ ì—†ìŒ')
            excel_filename = doc.metadata.get('source_file', 'Excel')
            header = f"ì°¸ê³  Q&A {i+1} (ì¶œì²˜: {excel_filename}, ì›ë³¸ ì§ˆë¬¸: \"{original_q[:50].strip()}...\")"
            content_preview = doc.page_content
        elif doc_type == "legal_document_chunk":
            source_file = doc.metadata.get('source_file', 'ì¶œì²˜ íŒŒì¼ ë¶ˆëª…')
            article_title = doc.metadata.get('article_title_full', 'ì „ì²´ ë¬¸ì„œì˜ ì¼ë¶€')
            header = f"ë²•ë¥  ë¬¸ì„œ {i+1}: '{source_file}'"
            if article_title not in ["ì „ì²´ ë¬¸ì„œì˜ ì¼ë¶€", "ì „ì²´ ë¬¸ì„œ", "ë‚´ìš© ì—†ìŒ", "N/A", "ì¡°í•­ ì •ë³´ ì—†ìŒ"]:
                header += f" (ê´€ë ¨ ì¡°í•­ ì¶”ì •: {article_title})"
            content_preview = doc.page_content.strip()
        else:
            source_file = doc.metadata.get('source_file', 'ì¶œì²˜ ë¶ˆëª…')
            header = f"ê¸°íƒ€ ë¬¸ì„œ {i+1} (ì¶œì²˜: {source_file})"
            content_preview = doc.page_content.strip()
        max_content_length = 700 # Increased slightly
        if len(content_preview) > max_content_length: content_preview = content_preview[:max_content_length] + "..."
        formatted_docs_list.append(f"{header}\nì¶”ì¶œ ë‚´ìš©:\n{content_preview}")
    return "\n\n---\n\n".join(formatted_docs_list)

def generate_answer_with_rag(question: str, vectorstore: Chroma, llm: ChatOllama) -> Dict[str, Any]:
    result_data = {"query": question, "result": "RAG ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ", "source_documents": []}
    try:
        retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5, "fetch_k": 20, "lambda_mult": 0.6})
        prompt = PromptTemplate(template=DEFAULT_RAG_PROMPT_TEMPLATE, input_variables=["context", "question"])
        rag_chain = (
            {"context": retriever | format_docs_for_context, "question": RunnablePassthrough()}
            | prompt | llm | StrOutputParser()
        )
        logging.info(f"Invoking RAG chain for question: '{question[:70]}...'")
        answer = rag_chain.invoke(question)
        retrieved_docs_for_display = retriever.invoke(question) # For display
        if not retrieved_docs_for_display: logging.warning("RAG retriever found no relevant documents (for display).")
        result_data.update({"result": answer, "source_documents": retrieved_docs_for_display})
        return result_data
    except Exception as e:
        logging.error(f"Error during RAG generation: {e}", exc_info=True)
        st.error(f"RAG ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        result_data["result"] = "RAG ë‹µë³€ ìƒì„± ê³¼ì •ì—ì„œ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        return result_data

def generate_direct_llm_answer(question: str, llm: ChatOllama) -> Dict[str, Any]:
    result_data = {"query": question, "result": "LLM ì§ì ‘ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ", "source_documents": None}
    try:
        direct_prompt_text = f"""ë‹¹ì‹ ì€ ë²•ë¥  ë¶„ì•¼ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. (ì™¸ë¶€ ë¬¸ì„œ/ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰ ê¸°ë°˜ ì•„ë‹˜)\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"""
        logging.info(f"Invoking direct LLM for question: '{question[:70]}...'")
        response = llm.invoke(direct_prompt_text)
        answer = response.content if isinstance(response, BaseMessage) else str(response)
        result_data["result"] = answer
        return result_data
    except Exception as e:
        logging.error(f"Error during direct LLM call: {e}", exc_info=True)
        st.error(f"LLM ì§ì ‘ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        result_data["result"] = "LLM ì§ì ‘ ë‹µë³€ ê³¼ì •ì—ì„œ ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ."
        return result_data

def generate_blended_answer(question: str, vectorstore: Chroma, llm: ChatOllama) -> Dict[str, Any]:
    logging.info(f"Blended Answer Step 1: RAG for: '{question[:70]}...'")
    rag_result_data = generate_answer_with_rag(question, vectorstore, llm)
    initial_rag_answer, source_documents = rag_result_data.get("result"), rag_result_data.get("source_documents")
    rag_had_error = "ì˜¤ë¥˜ ë°œìƒ" in (initial_rag_answer or "") or "ì‹œìŠ¤í…œ ì˜¤ë¥˜" in (initial_rag_answer or "")
    if rag_had_error or not initial_rag_answer:
        logging.warning(f"RAG step failed/produced error: '{initial_rag_answer}'."); return rag_result_data
    no_answer_phrases = ["ì œê³µëœ ë¬¸ë§¥ ì •ë³´ë§Œìœ¼ë¡œëŠ” ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "ë¬¸ë§¥ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
    if any(phrase in initial_rag_answer for phrase in no_answer_phrases):
        logging.info("RAG indicated no answer in context. Skipping refinement."); return rag_result_data
    logging.info("Blended Answer Step 2: LLM refinement.")
    retrieved_context_str = format_docs_for_context(source_documents if source_documents else [])
    refinement_prompt = PromptTemplate(template=REFINEMENT_PROMPT_TEMPLATE, input_variables=["original_question", "retrieved_context", "rag_answer"])
    refinement_chain = refinement_prompt | llm | StrOutputParser()
    try:
        final_answer = refinement_chain.invoke({"original_question": question, "retrieved_context": retrieved_context_str, "rag_answer": initial_rag_answer})
        return {"query": question, "result": final_answer, "source_documents": source_documents}
    except Exception as e:
        logging.error(f"Error during Blended Refinement: {e}", exc_info=True); st.error(f"ë‹µë³€ ê°œì„  ì¤‘ ì˜¤ë¥˜: {e}")
        st.warning("ë‹µë³€ ê°œì„  ì‹¤íŒ¨. ì´ˆê¸° RAG ë‹µë³€ í‘œì‹œ."); return rag_result_data

# --- Display Function ---
def display_results(result_data: Optional[Dict[str, Any]], answer_mode: str):
    if result_data is None: st.error("ê²°ê³¼ ë°ì´í„° ì—†ìŒ (None)."); logging.error("display_results called with None"); return
    if not isinstance(result_data, dict): st.error(f"ê²°ê³¼ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜ (type: {type(result_data)})."); logging.error(f"Invalid result_data: {result_data}"); return
    st.markdown(f"##### ğŸ¤– AI ë‹µë³€ ({answer_mode})")
    answer = result_data.get("result", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨/ê²°ê³¼ ì—†ìŒ.")
    if not answer or answer.strip() == "": st.warning("AI ìƒì„± ë‹µë³€ ë‚´ìš© ë¹„ì–´ìˆìŒ."); answer = "AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    st.markdown(answer); st.markdown("---")
    source_documents = result_data.get("source_documents")
    if source_documents and isinstance(source_documents, list):
        st.markdown("##### ğŸ“š ì°¸ê³  ë¬¸í—Œ (ê²€ìƒ‰ëœ ë¬¸ì„œ)")
        for i, doc in enumerate(source_documents):
            if not isinstance(doc, Document) or not hasattr(doc, 'metadata'): logging.warning(f"Source doc {i} invalid: {doc}"); st.warning(f"ì¶œì²˜ {i+1} í˜•ì‹ ì˜¤ë¥˜."); continue
            doc_type = doc.metadata.get('doc_type', 'unknown')
            expander_title = ""
            if doc_type == "qa_pair":
                original_q = doc.metadata.get('original_question', "ì§ˆë¬¸ ì—†ìŒ")
                excel_filename = doc.metadata.get('source_file', 'Excel')
                expander_title = f"ì°¸ê³  Q&A {i+1}: \"{original_q[:40].strip()}...\" (ì¶œì²˜: {excel_filename})"
            else: # legal_document_chunk or unknown
                source_file = doc.metadata.get('source_file', 'ì¶œì²˜ ë¶ˆëª…')
                article_title_full = doc.metadata.get('article_title_full', 'ì¡°í•­ ì •ë³´ ì—†ìŒ')
                expander_title = f"ë²•ë¥  ë¬¸ì„œ {i+1}: {source_file}"
                if article_title_full not in ['ì¡°í•­ ì •ë³´ ì—†ìŒ', 'N/A', 'ë‚´ìš© ì—†ìŒ', 'ì „ì²´ ë¬¸ì„œ', 'ì „ì²´ ë¬¸ì„œì˜ ì¼ë¶€']:
                    expander_title += f" - (ê´€ë ¨ ì¡°í•­ ì¶”ì •: {article_title_full})"
            with st.expander(expander_title):
                st.markdown(f"**ì¶”ì¶œëœ ë‚´ìš©:**"); st.text(doc.page_content if doc.page_content else "ë‚´ìš© ì—†ìŒ")
                display_meta = {k:v for k,v in doc.metadata.items() if k not in ['source_file', 'article_title_full', 'start_index', 'doc_type', 'original_question']}
                if display_meta: st.markdown(f"**ë©”íƒ€ë°ì´í„°:**"); st.json(display_meta)
    elif answer_mode in ["RAG (ë¬¸ì„œ ê¸°ë°˜)", "Blended (RAG + LLM ê°œì„ )"]:
        st.info("ì´ ë‹µë³€ ëª¨ë“œì—ì„œëŠ” ê·¼ê±° ì¡°í•­ì´ ì œê³µë  ìˆ˜ ìˆìœ¼ë‚˜, í˜„ì¬ ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

# --- Main Streamlit Application ---
def main():
    st.set_page_config(page_title="AI ë²•ë¥  ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ", layout="wide", initial_sidebar_state="expanded")
    st.title("âš–ï¸ AI ë²•ë¥  ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

    session_defaults = {
        "llm": None, "llm_model_name": DEFAULT_OLLAMA_LLM_MODEL,
        "embeddings": None, "embedding_provider_name_loaded": "Ollama", "embedding_model_name_loaded": DEFAULT_OLLAMA_EMBEDDING_MODEL,
        "vectorstore": None, "vectorstore_config_key_loaded": None,
        "db_status_message": "DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ê³  'DB ì¤€ë¹„' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.", "db_ready": False,
        "texts_with_sources_cache": None, "last_loaded_doc_dir": DEFAULT_DOC_DIRECTORY, "last_loaded_file_type": "txt",
        "uploaded_qa_file_name": None, "qa_documents_cache": None,
        "sidebar_doc_dir": DEFAULT_DOC_DIRECTORY, "sidebar_file_type": "txt", "sidebar_emb_provider": "Ollama",
        "sidebar_ollama_emb_model": DEFAULT_OLLAMA_EMBEDDING_MODEL, "sidebar_google_emb_model": DEFAULT_GOOGLE_EMBEDDING_MODEL,
        "sidebar_ollama_llm_model": DEFAULT_OLLAMA_LLM_MODEL, "sidebar_chunk_size": CHUNK_SIZE_DEFAULT,
        "sidebar_chunk_overlap": CHUNK_OVERLAP_DEFAULT, "sidebar_force_recreate_db": False,
    }
    for key, value in session_defaults.items():
        if key not in st.session_state: st.session_state[key] = value
    
    st.caption(f"LLM ({st.session_state.llm_model_name if st.session_state.llm else 'ë¡œë“œ ì•ˆë¨'}), "
               f"ì„ë² ë”© ({st.session_state.embedding_provider_name_loaded}: "
               f"{st.session_state.embedding_model_name_loaded if st.session_state.embeddings else 'ë¡œë“œ ì•ˆë¨'})")

    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        st.subheader("1. ë²•ë¥  ë¬¸ì„œ ë¡œë”©")
        st.session_state.sidebar_doc_dir = st.text_input("ë¬¸ì„œ ë””ë ‰í† ë¦¬", value=st.session_state.sidebar_doc_dir, key="doc_dir_sb")
        st.session_state.sidebar_file_type = st.selectbox("ë¬¸ì„œ íƒ€ì…", ["txt", "pdf"], index=["txt", "pdf"].index(st.session_state.sidebar_file_type), key="file_type_sb")

        st.subheader("2. ì„ë² ë”© ëª¨ë¸")
        st.session_state.sidebar_emb_provider = st.selectbox("ì œê³µì", ["Ollama", "Google"], index=["Ollama", "Google"].index(st.session_state.sidebar_emb_provider), key="emb_prov_sb")
        current_embedding_model_to_use = ""
        if st.session_state.sidebar_emb_provider == "Ollama":
            st.session_state.sidebar_ollama_emb_model = st.text_input("Ollama ëª¨ë¸ëª…", value=st.session_state.sidebar_ollama_emb_model, key="ollama_emb_sb", help=f"ì˜ˆ: {DEFAULT_OLLAMA_EMBEDDING_MODEL}")
            current_embedding_model_to_use = st.session_state.sidebar_ollama_emb_model
        elif st.session_state.sidebar_emb_provider == "Google":
            st.session_state.sidebar_google_emb_model = st.text_input("Google ëª¨ë¸ëª…", value=st.session_state.sidebar_google_emb_model, key="google_emb_sb", disabled=not GOOGLE_API_KEY, help=f"ì˜ˆ: {DEFAULT_GOOGLE_EMBEDDING_MODEL}")
            if not GOOGLE_API_KEY: st.warning("âš ï¸ GOOGLE_API_KEY ì—†ìŒ.")
            current_embedding_model_to_use = st.session_state.sidebar_google_emb_model

        st.subheader("3. LLM (ë‹µë³€ ìƒì„±)")
        st.session_state.sidebar_ollama_llm_model = st.text_input("Ollama LLM ëª¨ë¸ëª…", value=st.session_state.sidebar_ollama_llm_model, key="ollama_llm_sb", help=f"ì˜ˆ: {DEFAULT_OLLAMA_LLM_MODEL}")

        st.subheader("4. ë²¡í„° DB ì„¤ì •")
        st.session_state.sidebar_chunk_size = st.slider("Chunk Size", 200, 4000, st.session_state.sidebar_chunk_size, 50, key="chunk_size_sb")
        st.session_state.sidebar_chunk_overlap = st.slider("Chunk Overlap", 0, 1000, st.session_state.sidebar_chunk_overlap, 50, key="chunk_overlap_sb")
        
        st.subheader("5. ì´ì „ Q&A ë°ì´í„° (ì„ íƒ)")
        uploaded_qa_file_ui = st.file_uploader("Q&A ì°¸ê³ ìë£Œ Excel (.xlsx, .xls)", type=["xlsx", "xls"], key="qa_file_uploader_sb")
        
        st.session_state.sidebar_force_recreate_db = st.checkbox("DB ê°•ì œ ì¬ìƒì„±", value=st.session_state.sidebar_force_recreate_db, key="force_recreate_sb")


    if st.session_state.llm is None or st.session_state.llm_model_name != st.session_state.sidebar_ollama_llm_model:
        with st.spinner(f"Ollama LLM ({st.session_state.sidebar_ollama_llm_model}) ë¡œë“œ ì¤‘..."):
            llm_instance = get_ollama_llm(st.session_state.sidebar_ollama_llm_model)
            if llm_instance: st.session_state.llm, st.session_state.llm_model_name = llm_instance, st.session_state.sidebar_ollama_llm_model
            else:
                if st.session_state.llm: st.sidebar.error(f"ìƒˆ LLM ë¡œë“œ ì‹¤íŒ¨. ì´ì „ LLM({st.session_state.llm_model_name}) ì‚¬ìš©.")
                else: st.sidebar.error(f"LLM({st.session_state.sidebar_ollama_llm_model}) ë¡œë“œ ì‹¤íŒ¨.")
    if st.session_state.llm: st.sidebar.success(f"LLM ({st.session_state.llm_model_name}) ì¤€ë¹„ ì™„ë£Œ")
    else: st.sidebar.error(f"LLM ({st.session_state.sidebar_ollama_llm_model}) ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ì•ˆë¨.")

    col1, col2 = st.columns([1.2, 2]) # Adjusted column ratio
    with col1:
        st.subheader("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬")
        st.info(st.session_state.db_status_message)
        safe_sidebar_emb_model_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', current_embedding_model_to_use).replace('/', '_').replace(':','_')
        current_sidebar_db_config_key = (f"chroma_db_{st.session_state.sidebar_emb_provider.lower()}_{safe_sidebar_emb_model_name}_cz{st.session_state.sidebar_chunk_size}_co{st.session_state.sidebar_chunk_overlap}")
        if st.session_state.db_ready and st.session_state.vectorstore_config_key_loaded != current_sidebar_db_config_key:
            st.warning("âš ï¸ DB ì„¤ì • ë³€ê²½ë¨. DB ì¬ë¡œë“œ í•„ìš”.")

        button_text = f"'{st.session_state.sidebar_doc_dir}'({st.session_state.sidebar_file_type.upper()}) ë¡œë“œ ë° DB ì¤€ë¹„"
        if st.session_state.sidebar_force_recreate_db: button_text += " (ê°•ì œ ì¬ìƒì„±)"
        if st.button(button_text, key="prepare_db_main", use_container_width=True):
            st.session_state.db_ready = False; st.session_state.db_status_message = "DB ì²˜ë¦¬ ì‹œì‘..."
            embeddings_instance, emb_init_success = None, False
            with st.spinner(f"{st.session_state.sidebar_emb_provider} ì„ë² ë”© ({current_embedding_model_to_use}) ì´ˆê¸°í™”..."):
                if st.session_state.sidebar_emb_provider == "Ollama":
                    if current_embedding_model_to_use: embeddings_instance = get_ollama_embeddings(current_embedding_model_to_use)
                    else: st.error("Ollama ì„ë² ë”© ëª¨ë¸ëª… ë¯¸ì§€ì •.")
                elif st.session_state.sidebar_emb_provider == "Google":
                    if not GOOGLE_API_KEY: st.error("Google API í‚¤ ì—†ìŒ.")
                    elif current_embedding_model_to_use: embeddings_instance = get_google_embeddings(current_embedding_model_to_use)
                    else: st.error("Google ì„ë² ë”© ëª¨ë¸ëª… ë¯¸ì§€ì •.")
                if embeddings_instance: emb_init_success = True
            if not emb_init_success:
                st.session_state.db_status_message = "âŒ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨."; st.session_state.embeddings = None; st.rerun()

            st.session_state.embeddings = embeddings_instance
            st.session_state.embedding_provider_name_loaded = st.session_state.sidebar_emb_provider
            st.session_state.embedding_model_name_loaded = current_embedding_model_to_use
            logging.info(f"Embeddings set: {st.session_state.embedding_provider_name_loaded} - {st.session_state.embedding_model_name_loaded}")

            # Load Q&A documents
            current_qa_docs = []
            if uploaded_qa_file_ui is not None:
                if (st.session_state.uploaded_qa_file_name != uploaded_qa_file_ui.name) or \
                   st.session_state.qa_documents_cache is None or st.session_state.sidebar_force_recreate_db:
                    with st.spinner(f"'{uploaded_qa_file_ui.name}' Q&A ë°ì´í„° ë¡œë“œ..."):
                        loaded_qa_docs = load_qa_from_excel(uploaded_qa_file_ui)
                    if loaded_qa_docs: st.session_state.qa_documents_cache, st.session_state.uploaded_qa_file_name = loaded_qa_docs, uploaded_qa_file_ui.name; st.success(f"'{uploaded_qa_file_ui.name}'ì—ì„œ {len(loaded_qa_docs)}ê°œ Q&A ë¡œë“œ ì™„ë£Œ.")
                    else: st.session_state.qa_documents_cache, st.session_state.uploaded_qa_file_name = [], uploaded_qa_file_ui.name # Error handled in load_qa_from_excel
                current_qa_docs = st.session_state.qa_documents_cache
                if current_qa_docs and not ((st.session_state.uploaded_qa_file_name != uploaded_qa_file_ui.name) or st.session_state.qa_documents_cache is None or st.session_state.sidebar_force_recreate_db):
                     if uploaded_qa_file_ui.name == st.session_state.uploaded_qa_file_name : st.info(f"ìºì‹œëœ Q&A {len(current_qa_docs)}ê°œ ì‚¬ìš©: '{st.session_state.uploaded_qa_file_name}'")

            else: # No QA file uploaded this time
                if st.session_state.uploaded_qa_file_name is not None: st.info("Q&A Excel íŒŒì¼ ì œê±°ë¨.") # If there was one before
                st.session_state.qa_documents_cache, st.session_state.uploaded_qa_file_name = [], None
            current_qa_docs = st.session_state.qa_documents_cache if st.session_state.qa_documents_cache else []


            # Load primary documents
            texts_sources_for_db = None
            should_load_fresh_docs = (st.session_state.last_loaded_doc_dir != st.session_state.sidebar_doc_dir or
                                      st.session_state.last_loaded_file_type != st.session_state.sidebar_file_type or
                                      st.session_state.texts_with_sources_cache is None or st.session_state.sidebar_force_recreate_db)
            if should_load_fresh_docs:
                if not os.path.isdir(st.session_state.sidebar_doc_dir):
                    st.error(f"ë¬¸ì„œ ë””ë ‰í† ë¦¬ '{st.session_state.sidebar_doc_dir}' ì°¾ì„ ìˆ˜ ì—†ìŒ."); st.session_state.texts_with_sources_cache = None; st.rerun()
                load_func = load_text_documents_from_directory if st.session_state.sidebar_file_type == "txt" else load_pdf_documents_from_directory
                with st.spinner(f"'{st.session_state.sidebar_doc_dir}' ({st.session_state.sidebar_file_type.upper()}) ë¡œë“œ..."):
                    texts_sources_for_db = load_func(st.session_state.sidebar_doc_dir)
                if texts_sources_for_db:
                    st.session_state.texts_with_sources_cache, st.session_state.last_loaded_doc_dir, st.session_state.last_loaded_file_type = texts_sources_for_db, st.session_state.sidebar_doc_dir, st.session_state.sidebar_file_type
                else: st.warning(f"'{st.session_state.sidebar_doc_dir}' ({st.session_state.sidebar_file_type})ì—ì„œ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨."); st.session_state.texts_with_sources_cache = None; st.rerun()
            else: texts_sources_for_db = st.session_state.texts_with_sources_cache; st.info(f"ìºì‹œëœ ë²•ë¥  ë¬¸ì„œ {len(texts_sources_for_db)}ê°œ ì‚¬ìš©.")

            # Create/Load Vectorstore
            if (texts_sources_for_db or current_qa_docs) or not st.session_state.sidebar_force_recreate_db:
                with st.spinner("ë²¡í„° DB êµ¬ì„± ì¤‘... (ì‹œê°„ ì†Œìš”ë  ìˆ˜ ìˆìŒ)"):
                    vector_db_instance = create_or_load_vectorstore(texts_sources_for_db, current_qa_docs, st.session_state.embeddings,
                                                                    st.session_state.embedding_provider_name_loaded, st.session_state.embedding_model_name_loaded,
                                                                    st.session_state.sidebar_chunk_size, st.session_state.sidebar_chunk_overlap,
                                                                    st.session_state.sidebar_force_recreate_db)
                if vector_db_instance:
                    st.session_state.vectorstore, st.session_state.vectorstore_config_key_loaded, st.session_state.db_ready = vector_db_instance, current_sidebar_db_config_key, True
                    st.session_state.db_status_message = f"âœ… DB ì¤€ë¹„ ì™„ë£Œ: {st.session_state.embedding_model_name_loaded} (Chunk: {st.session_state.sidebar_chunk_size}/{st.session_state.sidebar_chunk_overlap})"
                    if st.session_state.sidebar_force_recreate_db: st.session_state.sidebar_force_recreate_db = False
                else: st.session_state.vectorstore, st.session_state.db_ready = None, False; st.session_state.db_status_message = "âŒ ë²¡í„° DB ì¤€ë¹„ ì‹¤íŒ¨. ë¡œê·¸ í™•ì¸."
            else: st.session_state.db_status_message = "âŒ DB ê°•ì œ ì¬ìƒì„±: ì¶”ê°€í•  ë²•ë¥ /Q&A ë°ì´í„° ì—†ìŒ."
            st.rerun()

    with col2:
        st.subheader("ğŸ’¬ ì§ˆì˜ì‘ë‹µ")
        answer_mode = st.radio("ë‹µë³€ ë°©ì‹:", ["RAG (ë¬¸ì„œ ê¸°ë°˜)", "Blended (RAG + LLM ê°œì„ )", "LLM ì§ì ‘ ë‹µë³€"], index=0, key="answer_mode_main", horizontal=True,
                               help=("**RAG**: ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€.\n**Blended**: RAG ë‹µë³€ì„ LLMì´ ê°œì„ .\n**LLM ì§ì ‘ ë‹µë³€**: LLM ìì²´ ì§€ì‹ìœ¼ë¡œ ë‹µë³€."))
        llm_ready = bool(st.session_state.llm)
        db_compatible = st.session_state.db_ready and st.session_state.vectorstore is not None and (st.session_state.vectorstore_config_key_loaded == current_sidebar_db_config_key)
        can_ask = False
        if not llm_ready: st.warning("LLM ë¯¸ì¤€ë¹„. ì‚¬ì´ë“œë°” ì„¤ì • í™•ì¸.")
        if answer_mode == "LLM ì§ì ‘ ë‹µë³€":
            if llm_ready: can_ask = True
        else: # RAG or Blended
            if not llm_ready: pass
            elif not st.session_state.db_ready: st.warning("DB ë¯¸ë¡œë“œ. ì™¼ìª½ 'DB ì¤€ë¹„' í´ë¦­.")
            elif not db_compatible: st.warning("ë¡œë“œëœ DBê°€ í˜„ì¬ ì„¤ì •ê³¼ ë‹¤ë¦„. DB ì¬ì¤€ë¹„ í•„ìš”.")
            else: can_ask = True

        question = st.text_area("ì§ˆë¬¸ ì…ë ¥:", height=100, key="user_question_main", placeholder="ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ ì…ë ¥...", disabled=not can_ask)
        if st.button("ë‹µë³€ ìƒì„± ğŸš€", key="ask_btn_main", disabled=not can_ask or not question.strip(), use_container_width=True):
            with st.spinner("AI ë‹µë³€ ìƒì„± ì¤‘... ğŸ¤”"):
                final_result_data = None
                if answer_mode != "LLM ì§ì ‘ ë‹µë³€" and not st.session_state.vectorstore: st.error("ë²¡í„°ìŠ¤í† ì–´ ë¯¸ì¤€ë¹„.")
                elif not st.session_state.llm: st.error("LLM ë¯¸ì¤€ë¹„.")
                else:
                    if answer_mode == "RAG (ë¬¸ì„œ ê¸°ë°˜)": final_result_data = generate_answer_with_rag(question, st.session_state.vectorstore, st.session_state.llm)
                    elif answer_mode == "Blended (RAG + LLM ê°œì„ )": final_result_data = generate_blended_answer(question, st.session_state.vectorstore, st.session_state.llm)
                    elif answer_mode == "LLM ì§ì ‘ ë‹µë³€": final_result_data = generate_direct_llm_answer(question, st.session_state.llm)
                if final_result_data: display_results(final_result_data, answer_mode)
        elif not question.strip() and can_ask: st.info("ë‹µë³€ ìƒì„±í•˜ë ¤ë©´ ì§ˆë¬¸ ì…ë ¥.")

if __name__ == "__main__":
    Path(DEFAULT_DOC_DIRECTORY).mkdir(parents=True, exist_ok=True)
    logging.info(f"Default document directory ensured: '{DEFAULT_DOC_DIRECTORY}'")
    main()